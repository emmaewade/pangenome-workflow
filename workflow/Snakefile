
########### Configuration  #############

configfile: "config/config.yaml" 

outname=config["outname"]
accortaxon=config["accortaxon"]
filename=config["filename"]
########################################
    
rule all:
    input:
        expand("results/{name}/roary/err.txt", name=config['outname']),
        expand("results/{name}/roary/out.txt", name=config['outname'])
        
rule download_preset: 
    output:
        gen = "workflow/data/assembly_summary_genbank.txt",
        ref = "workflow/data/assembly_summary_refseq.txt",
        names = "workflow/data/names.dmp",
        nodes = "workflow/data/nodes.dmp"
    threads:
        8
    #log: stdout="results/{outname}/logs/download_preset.stdout", stderr="results/{outname}/logs/download_preset.stderr"
    shell:
        '''
        echo
        echo "------------------------------------"
        echo "| 0/7 Downloading necessary data... |"
        echo "------------------------------------"
        echo 
        
        mkdir -p data
        mkdir -p data/preset
        
        echo "Downloading Genbank assembly summary..."
        curl -o --retry 10 {output.gen} "ftp://ftp.ncbi.nlm.nih.gov/genomes/ASSEMBLY_REPORTS/assembly_summary_genbank.txt"
        
        echo ""
        echo "Download RefSeq assembly summary..."
        curl -o --retry 10 {output.ref} "ftp://ftp.ncbi.nlm.nih.gov/genomes/refseq/assembly_summary_refseq.txt"
        
        echo ""
        echo "Downloading NCBI Taxonomy data..."
        curl -o --retry 10 workflow/data/taxdump.tar.gz "ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz"
        cd workflow/data
        tar -xzf taxdump.tar.gz names.dmp nodes.dmp
        rm taxdump.tar.gz
        '''   

rule make_table: 
    input:
        script = "workflow/scripts/make_table_main.py",
        gen = "workflow/data/assembly_summary_genbank.txt",
        ref = "workflow/data/assembly_summary_refseq.txt",
        names = "workflow/data/names.dmp",
        nodes = "workflow/data/nodes.dmp"
    output:
        'results/{outname}/master.csv',
        'results/{outname}/for_download.tsv'
    conda: 
        "envs/environment.yml"
    threads: 
        8
    params: 
        accortaxon={accortaxon},
        filename={filename},
        outname={outname}
    singularity:
        #"workflow/containers/first-environment.sif"
        "docker://catgumag/pandas-scibioxl:latest"
    shell:
        """
	    mkdir -p results
	    mkdir -p results/{outname}
	    
        echo
        echo "------------------------------"
        echo "| 1/6 Starting make table... |"
        echo "------------------------------"
        echo 
        
        python3 {input.script} \
        --accortaxon {params.accortaxon} \
        --filename {params.filename} \
        --outdir results/{params.outname} \
        --outfilename master \
        --gen {input.gen} \
        --ref {input.ref} \
        --names {input.names} \
        --nodes {input.nodes}
        """
        

rule download_gbff:
    input:
        master_interest = "results/{outname}/for_download.tsv"
    output:
        gbff_download = ("results/{outname}/summaries/gbff_sum.txt")
    params: 
        outname={outname}
    threads: 
        8
    shell:
        """
        
        echo 
        echo "---------------------------------"
        echo "| 2/6 Starting GBFF download... |"
        echo "---------------------------------"
        echo
        
        
        gbff_dir="results/{params.outname}/gbff-downloads"
        mkdir -p $gbff_dir
        
        bash workflow/scripts/bit-dl-ncbi-assemblies -f genbank -j {threads} -m {input.master_interest} -d $gbff_dir
        
        mkdir -p results/{params.outname}/summaries/
        ls $gbff_dir > {output.gbff_download}
        
        mv $gbff_dir/NCBI-accessions-not-downloaded.txt results/{params.outname}/NCBI-gbff-accessions-not-downloaded.txt
        
        #deleting uncompleted files due to server issues, gunzip -t tests file integrity
        echo "Moving incomplete files....\n"
        #find $gbff_dir -type f -exec sh -c 'if ! gunzip -t "{{}}" >/dev/null 2>&1; then echo "Deleting: {{}}"; rm -f "{{}}"; fi' \;
        #find $gbff_dir -type f -exec sh -c 'if ! gunzip -t "{{}}" >/dev/null 2>&1; then echo "{{}}" >> results/{params.outname}/incomplete_files.txt; echo "Deleting: {{}}"; rm -f "{{}}"; fi' \;
        mkdir -p results/{params.outname}/incomplete_files
        find $gbff_dir -type f -exec sh -c 'if ! gunzip -t "{{}}" >/dev/null 2>&1; then echo "{{}}" >> results/{params.outname}/incomplete_files.txt; echo "Moving: {{}}"; mv "{{}}" results/{params.outname}/incomplete_files; fi' \;
        """   
        
rule convert_gbff_and_fa:
    input:
        gb_to_gff3_script = "workflow/scripts/bp_genbank2gff3.pl",
        gb_to_fa_script = "workflow/scripts/convert_gb_to_fa.pl",
        gbff_summary = "results/{outname}/summaries/gbff_sum.txt"
    output:
        proc_summary = ("results/{outname}/summaries/convert.txt")
    params: 
        outname={outname}
    threads: 
        10
    singularity:
        #"workflow/containers/bioperl.sif"
        "docker://bioperl/bioperl:latest"
    shell:
        """
        
        echo
        echo "-----------------------------------"
        echo "| 3/6 Starting GBFF conversion... |"
        echo "-----------------------------------"
        echo
        
        # Input file containing FTP paths, one per line
        # Loop over each line in the input file
        gbff_dir="results/{params.outname}/gbff-downloads"
        processed_dir="results/{params.outname}/processed_files"
        mkdir -p $processed_dir

        filenames=$(find $gbff_dir -name *.gz -type f)
        
        N={threads}
        
        # Loop over each filename in the filtered directory listing
        for filename in $filenames
        do
        (
            perl {input.gb_to_gff3_script} $filename --outdir $processed_dir --quiet 
            perl {input.gb_to_fa_script} $filename $processed_dir
            sleep $(( (RANDOM % 3) + 1))
        
        ) &
    
            if [[ $(jobs -r -p | wc -l) -ge $N ]]; then
                # now there are $N jobs already running, so wait here for any job
                # to be finished so there is a place to start next one.
                wait -n
            fi
        
        done
        
        wait 
        
        ls $processed_dir > {output.proc_summary}
        """
        
'''
rule download_fna:
    input:
        master_file = 'results/{outname}/for_download.tsv'
    output:
        abyss_download = ("results/{outname}/summaries/fna_sum.txt")
    params: 
        outname={outname}
    threads: 
        8
    shell:
        """
        
        echo 
        echo "-----------------------------------"
        echo "| 3/6 Starting FASTA download... |"
        echo "-----------------------------------"
        echo
        
        
        fna_dir="results/{params.outname}/fna-downloads"
        mkdir -p $fna_dir
        
        bash workflow/scripts/bit-dl-ncbi-assemblies -f fasta -j {threads} -m {input.master_file} -d $fna_dir
        
        mkdir -p results/{params.outname}/summaries/
        ls $fna_dir > {output.abyss_download}
        
        mv $fna_dir/NCBI-accessions-not-downloaded.txt results/{params.outname}/NCBI-fna-accessions-not-downloaded.txt
        
        #Deleting uncompleted files due to server issues, gunzip -t tests file integrity
        echo "Moving incomplete files....\n"
        #find $fna_dir -type f -exec sh -c 'if ! gunzip -t "{{}}" >/dev/null 2>&1; then echo "Deleting: {{}}"; rm -f "{{}}"; fi' \;
        mkdir -p results/{params.outname}/incomplete_files
        find $fna_dir -type f -exec sh -c 'if ! gunzip -t "{{}}" >/dev/null 2>&1; then echo "{{}}" >> results/{params.outname}/incomplete_files.txt; echo "Moving: {{}}"; mv "{{}}" results/{params.outname}/incomplete_files ; fi' \;
        """   
'''

rule abyss_fac:
    input:
        master = "results/{outname}/master.csv",
        fna_summary = ("results/{outname}/summaries/convert.txt")
    output:
        abyss_out = "results/{outname}/abyss_fac_output.txt"
    params: 
        outname={outname}
    singularity:
        #"workflow/containers/abyss.sif"
        "docker://pegi3s/abyss:latest"
    shell:
        '''
        
        echo 
        echo "--------------------------------------"
        echo "| 4/6 Starting abyss-fac analysis... |"
        echo "--------------------------------------"
        echo
        
        touch {output.abyss_out}
        cd "results/{params.outname}/processed_files"
        find . -name '*.fa' | xargs -n 1000 bash -c 'abyss-fac -t1 "$@" ' > "../../../{output.abyss_out}"
        '''

rule add_abyss_to_master:
    input:
        master = "results/{outname}/master.csv",
        abyss_out = "results/{outname}/abyss_fac_output.txt",
        script = "workflow/scripts/join_abyss_fac_results.py"
    output:
        master_abyss = "results/{outname}/master_abyss.csv"
    singularity:
        #"workflow/containers/first-environment.sif"
        "docker://catgumag/pandas-scibioxl:latest"
    shell:
        '''
        python3 {input.script} --master {input.master} --abyss_out {input.abyss_out} --result {output.master_abyss}
        
        echo 
        echo "-----------------------------------------------------"
        echo "| 5/6 Added the abyss fac results to master file... |"
        echo "-----------------------------------------------------"
        echo
        
        '''

rule run_roary:
    input:
        proc_summary = ("results/{outname}/summaries/convert.txt"),
        master_abyss = "results/{outname}/master_abyss.csv"
    output:
        err = "results/{outname}/roary/err.txt",
        out = "results/{outname}/roary/out.txt"
    params: 
        outname={outname}
    threads: 
        8
    singularity:
        #"workflow/containers/roary_trusty.sif"
        "docker://sangerpathogens/roary:trusty"
    shell:
        """
        
        echo 
        echo "-------------------------"
        echo "| 6/6 Starting roary... |"
        echo "-------------------------"
        echo
        
        outdir='results/{outname}/roary'
        mkdir -p $outdir

        #-r = create R plots --> need to load R and ggplot2??
        #-p = number of threads
        #-e --mafft a multiFASTA alignment of core genes using MAFFT
        #-i = minimum percentage identity for blastp
        #-cd = percentage of isolates a gene must be in to be core
        # -f = output directory
        roary -r -p 30 -e --mafft -i 80 -cd 80 -f $outdir results/{params.outname}/processed_files/* > {output.out} 2> {output.err}
        echo "Finished roary..."
        """    
 

