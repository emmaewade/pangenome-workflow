'''
threads: 
    8
resources:
    slurm_partition= 
    runtime=
    constraint=
    mem_mb= 
    mem_mb_per_cpu=
    tasks=
    cpus_per_task=
    nodes=
    slurm_extra="
''' 


########### Configuration  #############

configfile: "workflow/profile/config.yaml"

outname=config["outname"]
accortaxon=config["accortaxon"]
filename=config["filename"]

########################################
    
rule all:
    input:
        expand("results/{name}/roary/err.txt", name=config['outname']),
        expand("results/{name}/roary/out.txt", name=config['outname'])
        
rule download_preset: 
    output:
        gen = "data/preset/assembly_summary_genbank.txt",
        ref = "data/preset/assembly_summary_refseq.txt",
        names = "data/preset/names.dmp",
        nodes = "data/preset/nodes.dmp"
    threads:
        8
    #log: stdout="results/{outname}/logs/download_preset.stdout", stderr="results/{outname}/logs/download_preset.stderr"
    shell:
        '''
        echo
        echo "------------------------------------"
        echo "| 0/7 Downloading necessary data... |"
        echo "------------------------------------"
        echo 
        
        mkdir -p data
        mkdir -p data/preset
        
        echo "Downloading Genbank assembly summary..."
        curl -o {output.gen} "ftp://ftp.ncbi.nlm.nih.gov/genomes/ASSEMBLY_REPORTS/assembly_summary_genbank.txt"
        
        echo ""
        echo "Download RefSeq assembly summary..."
        curl -o {output.ref} "ftp://ftp.ncbi.nlm.nih.gov/genomes/refseq/assembly_summary_refseq.txt"
        
        echo ""
        echo "Downloading NCBI Taxonomy data..."
        curl -o data/preset/taxdump.tar.gz "ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz"
        cd data/preset/
        tar -xzf taxdump.tar.gz names.dmp nodes.dmp
        rm taxdump.tar.gz
        '''   

rule make_table: 
    input:
        script = "workflow/scripts/make_table_main.py",
        gen = "data/preset/assembly_summary_genbank.txt",
        ref = "data/preset/assembly_summary_refseq.txt",
        names = "data/preset/names.dmp",
        nodes = "data/preset/nodes.dmp"
    output:
        'results/{outname}/master.csv',
    conda: 
        "envs/environment.yml"
    threads: 
        8
    #log: stdout="results/{outname}/logs/make_table.stdout", stderr="results/{outname}/logs/make_table.stderr"
    params: 
        accortaxon={accortaxon},
        filename={filename},
        outname={outname}
    singularity:
        "containers/first-environment.sif"
    shell:
        """
	    mkdir -p results
	    mkdir -p results/{outname}
	    
        echo
        echo "------------------------------"
        echo "| 1/7 Starting make table... |"
        echo "------------------------------"
        echo 
        
        python3 {input.script} \
        --accortaxon {params.accortaxon} \
        --filename {params.filename} \
        --outdir results/{params.outname} \
        --outfilename master
        
        """
        
rule get_interest_ftp: 
    input: 
        'results/{outname}/master.csv'
    output: 
        csv = 'results/{outname}/of_interest.csv',
        paths = 'results/{outname}/master_ftp_paths.txt'

    shell: 
        '''
        
        echo
        echo "----------------------------------------"
        echo "| 2/7 Finding FTP paths of interest... |"
        echo "----------------------------------------"
        echo
        
        awk -F',' 'NR==1 {{ for(i=1; i<=NF; i++) {{ if ($i== "interest_assembly_accession") {{ column=i; break }} }} }} $column != "" {{ print }}' '{input}' > '{output.csv}'
        awk -F',' 'NR==1 {{for (i=1; i<=NF; i++) {{ if ($i == "genbank_ftp_path") {{ column=i; break }} }} }} NR>1 {{ print $column }}' '{output.csv}' > '{output.paths}'
        '''
 
rule download_for_abyss_fac:
    input:
        ftp_paths = "results/{outname}/master_ftp_paths.txt" 
    output:
        abyss_summary = ("results/{outname}/summaries/fna_sum.txt")
    params: 
        outname={outname}
    threads: 
        8
    shell:
        """
        
        echo 
        echo "-----------------------------------"
        echo "| 3/7 Starting FASTA download... |"
        echo "-----------------------------------"
        echo
        
        
        # Input file containing FTP paths, one per line
        # Loop over each line in the input file
        fna_dir="results/{params.outname}/fna-downloads"
        mkdir -p $fna_dir
        
        N=8
        
        while read line; do

            # Download the directory listing using curl and filter it for *.fna files
            filenames=$(curl -s $line/ | grep -oP '(?<=href=")[^"]*genomic\.fna')
            
            # Loop over each filename in the filtered directory listing
            for filename in $filenames ; do 
                (
                # Download the file using curl
                curl -o $fna_dir/$filename".gz" $line/$filename".gz"
                #sleep $(( (RANDOM % 3) + 1))
                
                ) &
            
                if [[ $(jobs -r -p | wc -l) -ge $N ]]; then
                    # now there are $N jobs already running, so wait here for any job
                    # to be finished so there is a place to start next one.
                    wait -n
                fi
                
            done
            
        done < {input.ftp_paths}
        
        wait 
        
        mkdir -p results/{params.outname}/summaries/
        ls $fna_dir > {output.abyss_summary}
        """

rule abyss_fac:
    input:
        master = "results/{outname}/master.csv",
        fna_files = "results/{outname}/summaries/fna_sum.txt"
    output:
        abyss_out = "results/{outname}/abyss_fac_output.txt"
    params: 
        outname={outname}
    singularity:
        "containers/abyss.sif"
    shell:
        '''
        
        echo 
        echo "--------------------------------------"
        echo "| 4/7 Starting abyss-fac analysis... |"
        echo "--------------------------------------"
        echo
        
        touch {output.abyss_out}
        cd "results/{params.outname}/fna-downloads"
        ls
        find . -name '*gz' | xargs -n 1000 bash -c 'abyss-fac -t1 "$@" ' > "../../../{output.abyss_out}"
        '''

rule add_abyss_to_master:
    input:
        master = "results/{outname}/master.csv",
        abyss_out = "results/{outname}/abyss_fac_output.txt",
        script = "workflow/scripts/join_abyss_fac_results.py"
    output:
        master_abyss = "results/{outname}/master_abyss.csv"
    singularity:
        "containers/first-environment.sif"
    shell:
        '''
        python3 {input.script} --master {input.master} --abyss_out {input.abyss_out} --result {output.master_abyss}
        
        echo 
        echo "-----------------------------------------------------"
        echo "| 5/7 Added the abyss fac results to master file... |"
        echo "-----------------------------------------------------"
        echo
        
        '''
        
rule download_and_convert_gbff:
    input:
        script = "workflow/scripts/bp_genbank2gff3.pl",
        ftp_paths = "results/{outname}/master_ftp_paths.txt",
        master_abyss = "results/{outname}/master_abyss.csv"
    output:
        #gbff_dir = directory("results/gbff-downloads"),
        #processed_dir = directory("results/processed_files"),
        gbff_summary = ("results/{outname}/summaries/download_gbff.txt"),
        proc_summary = ("results/{outname}/summaries/convert.txt")
    conda:
        "envs/perl-env.yml" #need to check up on
    params: 
        outname={outname}
    threads: 
        8
    singularity:
        "containers/bioperl.sif"
    shell:
        """
        
        echo
        echo "----------------------------------------------------"
        echo "| 6/7 Starting GFF download and GBFF conversion... |"
        echo "---------------------------------------------------"
        echo
        
        # Input file containing FTP paths, one per line
        # Loop over each line in the input file
        gbff_dir="results/{params.outname}/gbff-downloads"
        processed_dir="results/{params.outname}/processed_files"
        mkdir -p $gbff_dir
        mkdir -p $processed_dir
        
        N=8
        
        while read line; do
            # Download the directory listing using curl and filter it for *.gbff files
            filenames=$(curl -s $line/ | grep -oP '(?<=href=")[^"]*genomic\.gbff')
            #filenames=$(curl -s $line/ | pcregrep -o '(?<=href=")[^"]*genomic\.gbff')
        
            # Loop over each filename in the filtered directory listing
            for filename in $filenames ; do
                (
                # Download the file using curl
                curl -o $gbff_dir/$filename".gz" $line/$filename".gz"
                # Convert gbff file to gff
                perl {input.script} $gbff_dir/"$filename".gz --outdir  $processed_dir --quiet
                ) &
        
                if [[ $(jobs -r -p | wc -l) -ge $N ]]; then
                    # now there are $N jobs already running, so wait here for any job
                    # to be finished so there is a place to start next one.
                    wait -n
                fi
            
            done
        
        done < {input.ftp_paths}
        
        wait
        
        ls $gbff_dir > {output.gbff_summary}
        ls $processed_dir > {output.proc_summary}
        """
        
rule run_roary:
    input:
        proc_summary = ("results/{outname}/summaries/convert.txt")
    output:
        err = "results/{outname}/roary/err.txt",
        out = "results/{outname}/roary/out.txt"
    conda:
        "envs/roary-env.yml"
    params: 
        outname={outname}
    threads: 
        8
    singularity:
        "containers/roary_trusty.sif"
    shell:
        """
        
        echo 
        echo "-------------------------"
        echo "| 7/7 Starting roary... |"
        echo "-------------------------"
        echo
        
        outdir='results/{outname}/roary'
        mkdir -p $outdir

        #-r = create R plots --> need to load R and ggplot2??
        #-p = number of threads
        #-e --mafft a multiFASTA alignment of core genes using MAFFT
        #-i = minimum percentage identity for blastp
        #-cd = percentage of isolates a gene must be in to be core
        # -f = output directory
        roary -r -p 30 -e --mafft -i 80 -cd 80 -f $outdir results/{params.outname}/processed_files/* > {output.out} 2> {output.err}
        echo "Finished roary..."
        """    
 
